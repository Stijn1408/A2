{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49b92301-b1cd-4b64-b196-643299825889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped the streaming query and the spark context\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession, Window, Row\n",
    "from pyspark.sql.functions import max as sparkMax\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "\n",
    "# Set up Spark configuration\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"Assignment2\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# Read BTC and Twitter data streams\n",
    "dfBTC = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka1:9093\") \\\n",
    "        .option(\"subscribe\", \"price\") \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .load()\n",
    "\n",
    "dfTwitter = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka1:9093\") \\\n",
    "        .option(\"subscribe\", \"twitter\") \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .load()\n",
    "\n",
    "# Decode BTC data stream and store values in new data frame with correct schema\n",
    "price = dfBTC.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "dataSchema = StructType(\n",
    "        [StructField(\"Batch\", StringType(), True),\n",
    "         StructField(\"Timestamp\", StringType(), True),\n",
    "         StructField(\"Symbol\", StringType(), True),\n",
    "         StructField(\"Price\", StringType(), True),\n",
    "         StructField(\"Percent_change_1h\", StringType(), True),\n",
    "         ])\n",
    "\n",
    "priceDf = price.select(from_json(col(\"value\"), dataSchema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "priceDf2 = priceDf \\\n",
    "    .withColumn(\"Batch\",col(\"Batch\").cast(IntegerType())) \\\n",
    "    .withColumn(\"Timestamp\",col(\"Timestamp\").cast(StringType())) \\\n",
    "    .withColumn(\"Symbol\",col(\"Symbol\").cast(StringType())) \\\n",
    "    .withColumn(\"Price\",col(\"Price\").cast(FloatType())) \\\n",
    "    .withColumn(\"Percent_change_1h\",col(\"Percent_change_1h\").cast(FloatType()))\n",
    "\n",
    "# Decode Twitter data stream and store values in new data frame with correct schema\n",
    "twitter = dfTwitter.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "dataSchemaTwitter = StructType(\n",
    "        [StructField(\"Time\", StringType(), True),\n",
    "         StructField(\"Text\", StringType(), True),\n",
    "         StructField(\"Positive\", StringType(), True),\n",
    "         StructField(\"Neutral\", StringType(), True),\n",
    "         StructField(\"Negative\", StringType(), True),\n",
    "         ])\n",
    "\n",
    "twitterDf = twitter.select(from_json(col(\"value\"), dataSchemaTwitter).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "twitterDf2 = twitterDf \\\n",
    "    .withColumn(\"Time\",col(\"Time\").cast(TimestampType())) \\\n",
    "    .withColumn(\"Text\",col(\"Text\").cast(StringType())) \\\n",
    "    .withColumn(\"Positive\",col(\"Positive\").cast(IntegerType())) \\\n",
    "    .withColumn(\"Neutral\",col(\"Neutral\").cast(IntegerType())) \\\n",
    "    .withColumn(\"Negative\",col(\"Negative\").cast(IntegerType())) \n",
    "\n",
    "# Setup configuration to enable use of Google Cloud Storage\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "# Use Google Cloud Storage bucket for temporary BigQuery export data used by the connector.\n",
    "bucket = \"model_repo_de2021marleen\"\n",
    "spark.conf.set('temporaryGcsBucket', bucket)\n",
    "\n",
    "# Batch function for BTC data: select most expensive currency per batch number\n",
    "# Also compute time windows of 60 seconds to be able to join observations later in Google Data Studio\n",
    "# Then, send data to Google Big Query table\n",
    "def foreach_batch_functionBTC(dfBTC, batch_id):\n",
    "    windowBatch = Window.partitionBy(col(\"Batch\")).orderBy(col(\"Price\").desc())\n",
    "    priceWindow = dfBTC.withColumn(\"priceRank\", dense_rank().over(windowBatch))\n",
    "\n",
    "    highestPrice = priceWindow.where(priceWindow.priceRank == 1)\n",
    "    \n",
    "    finalBTC = highestPrice.withColumn(\"event_window\", window(col(\"Timestamp\"), \"60 seconds\"))\n",
    "    \n",
    "    finalBTC.write.format('bigquery') \\\n",
    "      .option('table', 'de2021-325314.assignment2.demoBTC') \\\n",
    "      .mode('append') \\\n",
    "      .save() \n",
    "\n",
    "# Batch function for Twitter data: sum positive, neutral and negative tweets per micro-batch of one minute\n",
    "# Then, send batch to Google Big Query table\n",
    "def foreach_batch_functionTwitter(dfTwitter, batch_id):\n",
    "    pos = dfTwitter.groupBy(window(\"Time\", \"60 seconds\")).agg(sum(\"Positive\").alias(\"Positive\"))\n",
    "    neu = dfTwitter.groupBy(window(\"Time\", \"60 seconds\")).agg(sum(\"Neutral\").alias(\"Neutral\"))\n",
    "    neg = dfTwitter.groupBy(window(\"Time\", \"60 seconds\")).agg(sum(\"Negative\").alias(\"Negative\"))\n",
    "\n",
    "    finalTwitter = pos.join(neu, [\"window\"]).join(neg, [\"window\"])\n",
    "        \n",
    "    finalTwitter.write.format('bigquery') \\\n",
    "      .option('table', 'de2021-325314.assignment2.demoTwitter') \\\n",
    "      .mode('append') \\\n",
    "      .save()\n",
    "\n",
    "\n",
    "# Write to a sink: the output is written to a Big Query Table\n",
    "# Create two queries, one for BTC, one for Twitter, and trigger a micro-batch every 60 seconds\n",
    "BTCQuery = priceDf2.writeStream \\\n",
    "                        .outputMode(\"append\") \\\n",
    "                        .trigger(processingTime = '60 seconds') \\\n",
    "                        .foreachBatch(foreach_batch_functionBTC) \\\n",
    "                        .start()\n",
    "\n",
    "twitterQuery = twitterDf2.writeStream \\\n",
    "                        .outputMode(\"append\") \\\n",
    "                        .trigger(processingTime = '60 seconds') \\\n",
    "                        .foreachBatch(foreach_batch_functionTwitter) \\\n",
    "                        .start()\n",
    "\n",
    "# Start queries. Stop queries and spark context after keyboard interrupt\n",
    "try:\n",
    "    BTCQuery.awaitTermination()\n",
    "    twitterQuery.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    BTCQuery.stop()\n",
    "    twitterQuery.stop()\n",
    "\n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Stopped the streaming query and the spark context\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
